{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7fba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the CIFAR-10 dataset with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec78dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_axial_rope(embeddings, height, width, dim, trainable=False, theta_h=None, theta_w=None):\n",
    "    batch_size_heads = embeddings.size(0)\n",
    "    seq_len = embeddings.size(1)  # Ensure seq_len is derived from embeddings\n",
    "    quarter_dim = dim // 4\n",
    "\n",
    "    x1 = embeddings[..., ::2]  # Even-indexed dimensions\n",
    "    x2 = embeddings[..., 1::2]  # Odd-indexed dimensions\n",
    "\n",
    "    if trainable:\n",
    "        assert theta_h is not None and theta_w is not None, \"Trainable theta values must be provided.\"\n",
    "    else:\n",
    "        theta_h = torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)], device=embeddings.device)\n",
    "        theta_w = torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)], device=embeddings.device)\n",
    "\n",
    "    pos_h = torch.arange(height, device=embeddings.device).unsqueeze(1)\n",
    "    pos_w = torch.arange(width, device=embeddings.device).unsqueeze(1)\n",
    "\n",
    "    sin_h = torch.sin(pos_h * theta_h)\n",
    "    cos_h = torch.cos(pos_h * theta_h)\n",
    "    sin_w = torch.sin(pos_w * theta_w)\n",
    "    cos_w = torch.cos(pos_w * theta_w)\n",
    "\n",
    "    rot_matrix_h = torch.zeros(height, quarter_dim, quarter_dim, device=embeddings.device)\n",
    "    rot_matrix_w = torch.zeros(width, quarter_dim, quarter_dim, device=embeddings.device)\n",
    "\n",
    "    for i in range(quarter_dim):\n",
    "        rot_matrix_h[:, i, i] = cos_h[:, i]\n",
    "        rot_matrix_h[:, (i + 1) % quarter_dim, i] = -sin_h[:, i]\n",
    "        rot_matrix_h[:, i, (i + 1) % quarter_dim] = sin_h[:, i]\n",
    "        rot_matrix_h[:, (i + 1) % quarter_dim, (i + 1) % quarter_dim] = cos_h[:, i]\n",
    "\n",
    "        rot_matrix_w[:, i, i] = cos_w[:, i]\n",
    "        rot_matrix_w[:, (i + 1) % quarter_dim, i] = -sin_w[:, i]\n",
    "        rot_matrix_w[:, i, (i + 1) % quarter_dim] = sin_w[:, i]\n",
    "        rot_matrix_w[:, (i + 1) % quarter_dim, (i + 1) % quarter_dim] = cos_w[:, i]\n",
    "\n",
    "    x1_h_rotated = torch.einsum('...q,hij->...ij', x1, rot_matrix_h)\n",
    "    x1_w_rotated = torch.einsum('...q,wij->...ij', x1, rot_matrix_w)\n",
    "    x2_h_rotated = torch.einsum('...q,hij->...ij', x2, rot_matrix_h)\n",
    "    x2_w_rotated = torch.einsum('...q,wij->...ij', x2, rot_matrix_w)\n",
    "\n",
    "    rotated_embeddings = torch.empty(x1_h_rotated.size()[:-1] + (x1_h_rotated.size(-1) * 2,), device=x1_h_rotated.device)\n",
    "    rotated_embeddings[..., ::2] = x1_h_rotated + x1_w_rotated\n",
    "    rotated_embeddings[..., 1::2] = x2_h_rotated + x2_w_rotated\n",
    "\n",
    "    rotated_embeddings = rotated_embeddings.view(batch_size_heads, seq_len, dim)\n",
    "    return rotated_embeddings\n",
    "\n",
    "class AxialRoPE(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        quarter_dim = dim // 4\n",
    "        self.theta_h = nn.Parameter(torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)]))\n",
    "        self.theta_w = nn.Parameter(torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)]))\n",
    "\n",
    "    def forward(self, embeddings, height, width):\n",
    "        return apply_axial_rope(embeddings, height, width, dim=embeddings.size(-1), trainable=True, theta_h=self.theta_h, theta_w=self.theta_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c631fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mixed_rope(embeddings, height, width, dim, theta_h, theta_w):\n",
    "    batch_size_heads = embeddings.size(0)\n",
    "    seq_len = embeddings.size(1)  # Ensure seq_len is derived from embeddings\n",
    "\n",
    "    quarter_dim = dim // 4\n",
    "\n",
    "    # rename embeddings to x for consistency with axial rope code\n",
    "    x = embeddings\n",
    "\n",
    "    pos_h = torch.arange(height, device=embeddings.device).unsqueeze(1)\n",
    "    pos_w = torch.arange(width, device=embeddings.device).unsqueeze(1)\n",
    "\n",
    "    # Compute sine and cosine values\n",
    "    sin_h = torch.sin(pos_h * theta_h)\n",
    "    cos_h = torch.cos(pos_h * theta_h)\n",
    "    sin_w = torch.sin(pos_w * theta_w)\n",
    "    cos_w = torch.cos(pos_w * theta_w)\n",
    "\n",
    "    # Construct rotational matrices\n",
    "    rot_matrix_h = torch.zeros(height, quarter_dim, quarter_dim, device=embeddings.device)\n",
    "    rot_matrix_w = torch.zeros(width, quarter_dim, quarter_dim, device=embeddings.device)\n",
    "\n",
    "    for i in range(quarter_dim):\n",
    "        rot_matrix_h[:, i, i] = cos_h[:, i]\n",
    "        rot_matrix_h[:, (i + 1) % quarter_dim, i] = -sin_h[:, i]\n",
    "        rot_matrix_h[:, i, (i + 1) % quarter_dim] = sin_h[:, i]\n",
    "        rot_matrix_h[:, (i + 1) % quarter_dim, (i + 1) % quarter_dim] = cos_h[:, i]\n",
    "\n",
    "        rot_matrix_w[:, i, i] = cos_w[:, i]\n",
    "        rot_matrix_w[:, (i + 1) % quarter_dim, i] = -sin_w[:, i]\n",
    "        rot_matrix_w[:, i, (i + 1) % quarter_dim] = sin_w[:, i]\n",
    "        rot_matrix_w[:, (i + 1) % quarter_dim, (i + 1) % quarter_dim] = cos_w[:, i]\n",
    "\n",
    "    # Rotate x1 and x2\n",
    "    try:\n",
    "        x_h_rotated = torch.einsum('...q,hij->...ij', x, rot_matrix_h)\n",
    "        x_w_rotated = torch.einsum('...q,wij->...ij', x, rot_matrix_w)\n",
    "        rotation_matrix = x_h_rotated + x_w_rotated\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error during rotation: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Combine and interleave rotated embeddings\n",
    "    rotated_embeddings = torch.empty(\n",
    "        x_h_rotated.size()[:-1] + (x_h_rotated.size(-1) * 2,),  # Double the last dimension size\n",
    "        device=x1_h_rotated.device\n",
    "    )\n",
    "    rotated_embeddings = x_h_rotated + x_w_rotated\n",
    "\n",
    "    # Reshape back to [batch_size_heads, seq_len, dim]\n",
    "    rotated_embeddings = rotated_embeddings.view(batch_size_heads, seq_len, dim)\n",
    "    return rotated_embeddings\n",
    "\n",
    "class RoPEMixed(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        quarter_dim = dim // 4\n",
    "        self.theta_h = nn.Parameter(torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)]))\n",
    "        self.theta_w = nn.Parameter(torch.tensor([100 ** (-i / quarter_dim) for i in range(quarter_dim)]))\n",
    "\n",
    "    def forward(self, embeddings, height, width):\n",
    "        return apply_axial_rope_mixed(\n",
    "            embeddings, height, width, dim=embeddings.size(-1),\n",
    "            theta_h=self.theta_h, theta_w=self.theta_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b89a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttention Module\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, height=4, width=4, axialRoPE=False, trainable_theta=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.axialRoPE = axialRoPE\n",
    "        self.trainable_theta = trainable_theta\n",
    "        self.axial_rope = AxialRoPE(dim // heads) if (axialRoPE and trainable_theta) else None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "        head_dim = embedding_dim // self.heads\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.view(batch_size, seq_len, 3, self.heads, head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, head_dim)\n",
    "        k = k.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, head_dim)\n",
    "        v = v.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, head_dim)\n",
    "\n",
    "        if self.axialRoPE:\n",
    "            if self.trainable_theta:\n",
    "                q = self.axial_rope(q, self.height, self.width)\n",
    "                k = self.axial_rope(k, self.height, self.width)\n",
    "            else:\n",
    "                q = apply_axial_rope(q, self.height, self.width, head_dim)\n",
    "                k = apply_axial_rope(k, self.height, self.width, head_dim)\n",
    "\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * self.scale\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.bmm(attn_probs, v)\n",
    "        attn_output = attn_output.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, mlp_dim=128, \n",
    "                 axialRoPE=False, trainable_theta=False, basic=False):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size  \n",
    "        self.num_patches = (image_size // patch_size) ** 2  # Calculate number of patches\n",
    "        self.patch_dim = patch_size * patch_size * 3  # Account for RGB channels\n",
    "        self.axialRoPE = axialRoPE \n",
    "        self.trainable_theta = trainable_theta \n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, dim)  # Embedding for each patch\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))  # Class token as a learnable parameter\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim))  # Positional embeddings\n",
    "\n",
    "        self.basic = basic  # Use of basic flag to control the inclusion of RoPE or Performers\n",
    "\n",
    "        if not self.basic:\n",
    "            # Define transformer blocks with additional features if not basic\n",
    "            self.transformer_layers = nn.ModuleList([\n",
    "                TransformerBlock(dim, heads, mlp_dim, self.patch_size, self.patch_size, \n",
    "                                 axialRoPE=axialRoPE, trainable_theta=trainable_theta) for _ in range(depth)\n",
    "            ])\n",
    "        else:\n",
    "            # Define simpler transformer blocks without RoPE or Performers\n",
    "            self.transformer_layers = nn.ModuleList([\n",
    "                TransformerBlock(dim, heads, mlp_dim, height=0, width=0, \n",
    "                                 axialRoPE=False, trainable_theta=False) for _ in range(depth)  \n",
    "                # Assuming simpler block configuration\n",
    "            ])\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),  # Normalization\n",
    "            nn.Linear(dim, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(batch_size, self.num_patches, -1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.mlp_head(x[:, 0])  # Output from the class token position\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, height, width, axialRoPE=False, trainable_theta=False):\n",
    "        \"\"\"\n",
    "        Transformer block with optional RoPE.\n",
    "        - `height` and `width` are ignored if this is a basic transformer (height = width = 0).\n",
    "        - `trainable`: Determines if the RoPE is trainable.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.msa_norm = nn.LayerNorm(dim)\n",
    "        self.axialRoPE = axialRoPE \n",
    "        self.trainable_theta = trainable_theta \n",
    "        \n",
    "        self.msa = SelfAttention(dim, heads, height, width, axialRoPE=axialRoPE, trainable_theta=trainable_theta)\n",
    "\n",
    "        self.mlp_norm = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply LayerNorm before the self-attention mechanism\n",
    "        x = x + self.msa(self.msa_norm(x))\n",
    "\n",
    "        # Apply a feed-forward network defined in MLP block\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d98450a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt Orthogonalization for generating orthogonal random projections\n",
    "def gram_schmidt(vectors):\n",
    "    orthogonalized = []\n",
    "    for v in vectors:\n",
    "        for u in orthogonalized:\n",
    "            v -= torch.dot(v, u) * u\n",
    "        v = v / v.norm()  # Normalize to unit length\n",
    "        orthogonalized.append(v)\n",
    "    return torch.stack(orthogonalized)\n",
    "\n",
    "# Generate Orthogonal Random Projections\n",
    "def generate_orthogonal_random_projections(dim, m):\n",
    "    random_matrix = torch.randn((m, dim), device=device)\n",
    "    orthogonal_matrix = gram_schmidt(random_matrix)\n",
    "    return orthogonal_matrix.T  # Return as (dim, m) for projection\n",
    "\n",
    "# Phi+ kernel implementation\n",
    "def phi_plus(z, m):\n",
    "    batch_size, seq_len, dim = z.size()\n",
    "    norm_squared = torch.norm(z, dim=-1, keepdim=True) ** 2\n",
    "    orthogonal_random_matrix = generate_orthogonal_random_projections(dim, m)\n",
    "    projected = z @ orthogonal_random_matrix  # [batch_size, seq_len, m]\n",
    "    phi_plus_features = torch.exp(-norm_squared / 2) * torch.exp(projected) / torch.sqrt(torch.tensor(m, dtype=z.dtype, device=z.device))\n",
    "    return phi_plus_features\n",
    "\n",
    "# Phi++ kernel implementation\n",
    "def phi_plus_plus(z, m):\n",
    "    batch_size, seq_len, dim = z.size()\n",
    "    norm_squared = torch.norm(z, dim=-1, keepdim=True) ** 2\n",
    "    orthogonal_random_matrix = generate_orthogonal_random_projections(dim, m)\n",
    "    projected = z @ orthogonal_random_matrix  # [batch_size, seq_len, m]\n",
    "    phi_plus_plus_features = torch.exp(-norm_squared / 2) * torch.cat([\n",
    "        torch.exp(projected),\n",
    "        torch.exp(-projected)\n",
    "    ], dim=-1) / torch.sqrt(torch.tensor(2 * m, dtype=z.dtype, device=z.device))\n",
    "    return phi_plus_plus_features\n",
    "\n",
    "# Performer Self-Attention (Supports Orthogonal Random Features)\n",
    "class PerformerSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, height=4, width=4,m=8, use_phi_plus_plus=False, axialRoPE=False, trainable_theta=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.m = m\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.use_phi_plus_plus = use_phi_plus_plus\n",
    "        self.axialRoPE = axialRoPE\n",
    "        self.trainable_theta = trainable_theta\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        # If Axial RoPE and trainable theta are enabled\n",
    "        if self.axialRoPE and self.trainable_theta:\n",
    "            self.axial_rope = AxialRoPE(dim // heads)  # Define AxialRoPE with trainable parameters\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "        print(f'Input shape: {x.shape}')\n",
    "        initial_head_dim = embedding_dim // self.heads\n",
    "\n",
    "\n",
    "        qkv = self.qkv(x).view(batch_size, seq_len, 3, self.heads, initial_head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        print(f'Q, K, V shapes after split: {q.shape}, {k.shape}, {v.shape}')\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, initial_head_dim)\n",
    "        k = k.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, initial_head_dim)\n",
    "        v = v.permute(0, 2, 1, 3).reshape(batch_size * self.heads, seq_len, initial_head_dim)\n",
    "        \n",
    "        print(f'Reshaped Q, K, V: {q.shape}, {k.shape}, {v.shape}')\n",
    "        # Assuming phi_plus_plus was used\n",
    "        if self.use_phi_plus_plus:\n",
    "            q_prime = phi_plus_plus(q, self.m)\n",
    "            k_prime = phi_plus_plus(k, self.m)\n",
    "            actual_head_dim = q_prime.size(-1)  \n",
    "        else:\n",
    "            q_prime = phi_plus(q, self.m)\n",
    "            k_prime = phi_plus(k, self.m)\n",
    "            actual_head_dim = q_prime.size(-1) \n",
    "        print(f'Q_prime, K_prime after phi: {q_prime.shape}, {k_prime.shape}')\n",
    "        actual_head_dim = q_prime.size(-1)  # Get the last dimension size after transformations\n",
    "\n",
    "       # Optional Axial RoPE transformation\n",
    "        if self.axialRoPE:\n",
    "            if self.trainable_theta:\n",
    "                q_prime = self.axial_rope(q_prime, self.height, self.width)  # Applying trainable RoPE to queries\n",
    "                k_prime = self.axial_rope(k_prime, self.height, self.width)  # Applying trainable RoPE to keys\n",
    "            else:\n",
    "                q_prime = apply_axial_rope(q_prime, self.height, self.width, actual_head_dim)  # Applying non-trainable RoPE\n",
    "                k_prime = apply_axial_rope(k_prime, self.height, self.width, actual_head_dim)\n",
    "        print(f'Q_prime, K_prime after RoPE: {q_prime.shape}, {k_prime.shape}')\n",
    "       \n",
    "        kv = torch.matmul(k_prime.transpose(-2, -1), v)\n",
    "        qkv = torch.matmul(q_prime, kv)\n",
    "        print(f'Intermediate KV and QKV shapes: {kv.shape}, {qkv.shape}')\n",
    "\n",
    "        attn_output = qkv*self.scale\n",
    "        attn_output = attn_output.view(batch_size, self.heads, seq_len, qkv.size(-1))\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.dim)    \n",
    "        print(f'Final output shape: {attn_output.shape}')\n",
    "\n",
    "        return self.fc(attn_output)    \n",
    "    \n",
    "    \n",
    "# Gaussian Performer Self-Attention\n",
    "import torch.nn.functional as F\n",
    "class GaussianPerformerSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, height=4, width=4, axialRoPE=False, trainable_theta=False, learnable_G=False):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim = dim\n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.device = device\n",
    "        self.axialRoPE = axialRoPE \n",
    "        self.trainable_theta = trainable_theta \n",
    "        self.learnable_G = learnable_G\n",
    "        self.head_dim = dim // heads\n",
    "        if self.axialRoPE and self.trainable_theta:\n",
    "            self.axial_rope = AxialRoPE(dim // heads)  # Define AxialRoPE with trainable parameters\n",
    "        if self.learnable_G:\n",
    "            # Initializing G as a learnable parameter\n",
    "            self.G = nn.Parameter(torch.randn(self.head_dim, self.head_dim))\n",
    "        else:\n",
    "            # Fixed random G\n",
    "            self.G = torch.randn((self.head_dim, self.head_dim), requires_grad=False)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).view(batch_size, seq_len, 3, self.heads, self.head_dim)\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "\n",
    "        q = q.reshape(batch_size * self.heads, seq_len, self.head_dim)\n",
    "        k = k.reshape(batch_size * self.heads, seq_len, self.head_dim)\n",
    "        v = v.reshape(batch_size * self.heads, seq_len, self.head_dim)  \n",
    "\n",
    "        q = F.relu(q @ self.G)\n",
    "        k = F.relu(k @ self.G)\n",
    "        \n",
    "        if self.axialRoPE:\n",
    "            if self.trainable_theta:\n",
    "                q = self.axial_rope(q, self.height, self.width)\n",
    "                k = self.axial_rope(k, self.height, self.width)\n",
    "            else:\n",
    "                q = apply_axial_rope(q, self.height, self.width, head_dim)\n",
    "                k = apply_axial_rope(k, self.height, self.width, head_dim)\n",
    "\n",
    "\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * self.scale\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.bmm(attn_probs, v).view(batch_size, self.heads, seq_len, self.head_dim)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).reshape(batch_size, seq_len, embedding_dim)\n",
    "        return self.fc(attn_output)\n",
    "\n",
    "\n",
    "# Vision Transformer with Performer Self-Attention\n",
    "class VisionTransformerPerformer(nn.Module):\n",
    "    def __init__(self, image_size=32, patch_size=4, num_classes=10, dim=64, depth=6, heads=8, \n",
    "                 mlp_dim=128, m=8, \n",
    "                 use_phi_plus_plus=False, use_gaussian_performer=False, axialRoPE=False, trainable_theta=False\n",
    "                 , learnable_G=False ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size  \n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = patch_size * patch_size * 3  # Adjust for 3 color channels\n",
    "        self.axialRoPE = axialRoPE \n",
    "        self.trainable_theta = trainable_theta \n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, dim))\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, self.num_patches + 1, dim))\n",
    "\n",
    "        self.performer_layers = nn.ModuleList([\n",
    "            TransformerBlockPerformer(dim, heads, mlp_dim, m, use_phi_plus_plus, use_gaussian_performer\n",
    "                                     , axialRoPE=axialRoPE, trainable_theta=trainable_theta,\n",
    "                                     learnable_G=learnable_G) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(batch_size, self.num_patches, -1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        for layer in self.performer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "\n",
    "class TransformerBlockPerformer(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, m, use_phi_plus_plus, use_gaussian_performer, \n",
    "                 axialRoPE=False, trainable_theta=False, learnable_G=False):\n",
    "        \"\"\"\n",
    "        Initializes a Performer Transformer block with the option to use phi_plus or phi_plus_plus kernels.\n",
    "        \n",
    "        Parameters:\n",
    "        - dim (int): Dimension of the input features.\n",
    "        - heads (int): Number of attention heads.\n",
    "        - mlp_dim (int): Dimension of the feed-forward network.\n",
    "        - m (int): Number of random features for the Performer kernel.\n",
    "        - use_phi_plus_plus (bool): Flag to decide between using phi_plus or phi_plus_plus kernels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.axialRoPE = axialRoPE \n",
    "        self.trainable_theta = trainable_theta \n",
    "        self.msa_norm = nn.LayerNorm(dim)  # Layer normalization before the self-attention\n",
    "        if use_gaussian_performer:\n",
    "            self.msa = GaussianPerformerSelfAttention(dim, heads, \n",
    "                                                      axialRoPE=axialRoPE, trainable_theta=trainable_theta, \n",
    "                                                      learnable_G=learnable_G)\n",
    "        else:\n",
    "            self.msa = PerformerSelfAttention(dim, heads, m, use_phi_plus_plus=use_phi_plus_plus,\n",
    "                                              axialRoPE=axialRoPE, trainable_theta=trainable_theta)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.mlp_norm = nn.LayerNorm(dim)  # Layer normalization before the feed-forward network\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),  # First linear layer\n",
    "            nn.GELU(),                # GELU activation function\n",
    "            nn.Linear(mlp_dim, dim)   # Second linear layer to bring back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer block using Performer attention.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (Tensor): Input tensor of shape [batch_size, seq_len, dim]\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor: Output tensor of the same shape as input after passing through the transformer block.\n",
    "        \"\"\"\n",
    "        # Apply self-attention and add the result to the input (residual connection)\n",
    "        x = x + self.msa(self.msa_norm(x))\n",
    "\n",
    "        # Apply the feed-forward network and add the result to the previous output (residual connection)\n",
    "        x = x + self.mlp(self.mlp_norm(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a267fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory to save the models\n",
    "save_dir = \"./saved_models_CIFAR10\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save function\n",
    "def save_model(model, model_name):\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved at: {save_path}\")\n",
    "\n",
    "# Load function\n",
    "def load_model(model_class, model_name):\n",
    "    model = model_class()\n",
    "    load_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded from: {load_path}\")\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    inference_times = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            start_time = time.time()\n",
    "            preds = model(x)\n",
    "            end_time = time.time()\n",
    "            inference_times.append(end_time - start_time)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (preds.argmax(1) == y).sum().item()\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    accuracy = total_correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy, avg_inference_time\n",
    "\n",
    "# Training function with speed measurement\n",
    "def train_with_speed(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    batch_times = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        batch_times.append(end_time - start_time)\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (preds.argmax(1) == y).sum().item()\n",
    "    avg_batch_time = np.mean(batch_times)\n",
    "    accuracy = total_correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), accuracy, avg_batch_time\n",
    "\n",
    "# Evaluation framework\n",
    "def evaluate_framework(model, model_name, train_loader, test_loader, optimizer, criterion):\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    train_times, inference_times = [], []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc, train_time = train_with_speed(model, train_loader, optimizer, criterion)\n",
    "        test_loss, test_acc, avg_inference_time = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        train_times.append(train_time)\n",
    "        inference_times.append(avg_inference_time)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Train Time/Batch: {train_time:.4f}s, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, \"\n",
    "              f\"Inference Time/Batch: {avg_inference_time:.4f}s\")\n",
    "\n",
    "    # Calculate variance in accuracy\n",
    "    accuracy_variance = np.var(test_accuracies)\n",
    "\n",
    "    # Save the model\n",
    "    save_model(model, model_name)\n",
    "\n",
    "    # Summary of results\n",
    "    print(f\"\\nSummary for {model_name}:\")\n",
    "    print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
    "    print(f\"Accuracy Variance: {accuracy_variance:.4f}\")\n",
    "    print(f\"Average Training Time per Batch: {np.mean(train_times):.4f}s\")\n",
    "    print(f\"Average Inference Time per Batch: {np.mean(inference_times):.4f}s\")\n",
    "\n",
    "\n",
    "# Initialize criterion\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "88cfecb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_transformer_basic...\n",
      "Epoch 1: Train Loss: 1.8009, Train Acc: 0.3362, Train Time/Batch: 0.0480s, Test Loss: 1.5688, Test Acc: 0.4321, Inference Time/Batch: 0.0678s\n",
      "Epoch 2: Train Loss: 1.4855, Train Acc: 0.4593, Train Time/Batch: 0.0479s, Test Loss: 1.3998, Test Acc: 0.4873, Inference Time/Batch: 0.0675s\n",
      "Epoch 3: Train Loss: 1.3352, Train Acc: 0.5157, Train Time/Batch: 0.0482s, Test Loss: 1.3519, Test Acc: 0.5096, Inference Time/Batch: 0.0695s\n",
      "Epoch 4: Train Loss: 1.2283, Train Acc: 0.5576, Train Time/Batch: 0.0484s, Test Loss: 1.2297, Test Acc: 0.5509, Inference Time/Batch: 0.0674s\n",
      "Epoch 5: Train Loss: 1.1374, Train Acc: 0.5908, Train Time/Batch: 0.0482s, Test Loss: 1.1830, Test Acc: 0.5765, Inference Time/Batch: 0.0682s\n",
      "Epoch 6: Train Loss: 1.0584, Train Acc: 0.6207, Train Time/Batch: 0.0488s, Test Loss: 1.1697, Test Acc: 0.5820, Inference Time/Batch: 0.0675s\n",
      "Epoch 7: Train Loss: 0.9979, Train Acc: 0.6417, Train Time/Batch: 0.0482s, Test Loss: 1.1423, Test Acc: 0.5958, Inference Time/Batch: 0.0681s\n",
      "Epoch 8: Train Loss: 0.9371, Train Acc: 0.6659, Train Time/Batch: 0.0484s, Test Loss: 1.1420, Test Acc: 0.5973, Inference Time/Batch: 0.0683s\n",
      "Epoch 9: Train Loss: 0.8813, Train Acc: 0.6849, Train Time/Batch: 0.0483s, Test Loss: 1.1372, Test Acc: 0.6040, Inference Time/Batch: 0.0678s\n",
      "Epoch 10: Train Loss: 0.8303, Train Acc: 0.7066, Train Time/Batch: 0.0488s, Test Loss: 1.1726, Test Acc: 0.6022, Inference Time/Batch: 0.0678s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_transformer_basic.pth\n",
      "\n",
      "Summary for vision_transformer_basic:\n",
      "Final Test Accuracy: 0.6022\n",
      "Accuracy Variance: 0.0031\n",
      "Average Training Time per Batch: 0.0483s\n",
      "Average Inference Time per Batch: 0.0680s\n"
     ]
    }
   ],
   "source": [
    "# Basic Vision Transformer (No RoPE, No Performer)\n",
    "model_basic = VisionTransformer(basic=True).to(device)\n",
    "optimizer = optim.Adam(model_basic.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_basic, \"vision_transformer_basic\", train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9d402ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_transformer_axial_rope_non_trainable_theta...\n",
      "Epoch 1: Train Loss: 2.1849, Train Acc: 0.1857, Train Time/Batch: 0.1656s, Test Loss: 2.1115, Test Acc: 0.2130, Inference Time/Batch: 0.1518s\n",
      "Epoch 2: Train Loss: 2.0880, Train Acc: 0.2240, Train Time/Batch: 0.1622s, Test Loss: 2.1007, Test Acc: 0.2285, Inference Time/Batch: 0.1450s\n",
      "Epoch 3: Train Loss: 2.0673, Train Acc: 0.2355, Train Time/Batch: 0.1588s, Test Loss: 2.0479, Test Acc: 0.2445, Inference Time/Batch: 0.1428s\n",
      "Epoch 4: Train Loss: 2.0282, Train Acc: 0.2497, Train Time/Batch: 0.1559s, Test Loss: 2.0356, Test Acc: 0.2425, Inference Time/Batch: 0.1409s\n",
      "Epoch 5: Train Loss: 2.0155, Train Acc: 0.2542, Train Time/Batch: 0.1565s, Test Loss: 2.0091, Test Acc: 0.2557, Inference Time/Batch: 0.1410s\n",
      "Epoch 6: Train Loss: 2.0060, Train Acc: 0.2625, Train Time/Batch: 0.1545s, Test Loss: 2.0127, Test Acc: 0.2508, Inference Time/Batch: 0.1491s\n",
      "Epoch 7: Train Loss: 2.0104, Train Acc: 0.2578, Train Time/Batch: 0.1545s, Test Loss: 2.0187, Test Acc: 0.2579, Inference Time/Batch: 0.1395s\n",
      "Epoch 8: Train Loss: 2.0303, Train Acc: 0.2517, Train Time/Batch: 0.1526s, Test Loss: 2.0351, Test Acc: 0.2459, Inference Time/Batch: 0.1365s\n",
      "Epoch 9: Train Loss: 2.0156, Train Acc: 0.2589, Train Time/Batch: 0.1484s, Test Loss: 2.0045, Test Acc: 0.2654, Inference Time/Batch: 0.1376s\n",
      "Epoch 10: Train Loss: 1.9997, Train Acc: 0.2629, Train Time/Batch: 0.1519s, Test Loss: 1.9796, Test Acc: 0.2722, Inference Time/Batch: 0.1373s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_transformer_axial_rope_non_trainable_theta.pth\n",
      "\n",
      "Summary for vision_transformer_axial_rope_non_trainable_theta:\n",
      "Final Test Accuracy: 0.2722\n",
      "Accuracy Variance: 0.0003\n",
      "Average Training Time per Batch: 0.1561s\n",
      "Average Inference Time per Batch: 0.1422s\n"
     ]
    }
   ],
   "source": [
    "# Axial RoPE (Non-trainable Theata) with Vision Transformer\n",
    "model_non_trainable_axial_rope = VisionTransformer(axialRoPE=True, trainable_theta=False).to(device)\n",
    "optimizer = optim.Adam(model_non_trainable_axial_rope.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_non_trainable_axial_rope, \"vision_transformer_axial_rope_non_trainable_theta\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "dc41c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_transformer_axial_rope_trainable_theta...\n",
      "Epoch 1: Train Loss: 2.1830, Train Acc: 0.1879, Train Time/Batch: 0.1848s, Test Loss: 2.0856, Test Acc: 0.2267, Inference Time/Batch: 0.1477s\n",
      "Epoch 2: Train Loss: 2.0670, Train Acc: 0.2350, Train Time/Batch: 0.1863s, Test Loss: 2.0272, Test Acc: 0.2529, Inference Time/Batch: 0.1479s\n",
      "Epoch 3: Train Loss: 2.0583, Train Acc: 0.2411, Train Time/Batch: 0.1843s, Test Loss: 2.0497, Test Acc: 0.2454, Inference Time/Batch: 0.1436s\n",
      "Epoch 4: Train Loss: 2.0218, Train Acc: 0.2557, Train Time/Batch: 0.1839s, Test Loss: 2.0025, Test Acc: 0.2685, Inference Time/Batch: 0.1443s\n",
      "Epoch 5: Train Loss: 2.0011, Train Acc: 0.2644, Train Time/Batch: 0.1788s, Test Loss: 2.0000, Test Acc: 0.2660, Inference Time/Batch: 0.1403s\n",
      "Epoch 6: Train Loss: 2.0061, Train Acc: 0.2601, Train Time/Batch: 0.1780s, Test Loss: 2.0076, Test Acc: 0.2614, Inference Time/Batch: 0.1429s\n",
      "Epoch 7: Train Loss: 2.0194, Train Acc: 0.2546, Train Time/Batch: 0.1828s, Test Loss: 2.0025, Test Acc: 0.2665, Inference Time/Batch: 0.1408s\n",
      "Epoch 8: Train Loss: 1.9906, Train Acc: 0.2685, Train Time/Batch: 0.1871s, Test Loss: 2.0020, Test Acc: 0.2634, Inference Time/Batch: 0.1395s\n",
      "Epoch 9: Train Loss: 1.9915, Train Acc: 0.2694, Train Time/Batch: 0.1830s, Test Loss: 1.9747, Test Acc: 0.2762, Inference Time/Batch: 0.1386s\n",
      "Epoch 10: Train Loss: 1.9738, Train Acc: 0.2739, Train Time/Batch: 0.1805s, Test Loss: 1.9992, Test Acc: 0.2664, Inference Time/Batch: 0.1369s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_transformer_axial_rope_trainable_theta.pth\n",
      "\n",
      "Summary for vision_transformer_axial_rope_trainable_theta:\n",
      "Final Test Accuracy: 0.2664\n",
      "Accuracy Variance: 0.0002\n",
      "Average Training Time per Batch: 0.1830s\n",
      "Average Inference Time per Batch: 0.1423s\n"
     ]
    }
   ],
   "source": [
    "# Axial RoPE (Trainable Theata) with Vision Transformer\n",
    "model_trainable_axial_rope = VisionTransformer(axialRoPE=True, trainable_theta=True).to(device)\n",
    "optimizer = optim.Adam(model_trainable_axial_rope.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_trainable_axial_rope, \"vision_transformer_axial_rope_trainable_theta\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8789fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_performer_phi_plus_noROPE...\n",
      "Epoch 1: Train Loss: 1.9038, Train Acc: 0.2924, Train Time/Batch: 0.0548s, Test Loss: 1.7706, Test Acc: 0.3471, Inference Time/Batch: 0.0462s\n",
      "Epoch 2: Train Loss: 1.6280, Train Acc: 0.3970, Train Time/Batch: 0.0550s, Test Loss: 1.5214, Test Acc: 0.4372, Inference Time/Batch: 0.0465s\n",
      "Epoch 3: Train Loss: 1.4810, Train Acc: 0.4528, Train Time/Batch: 0.0546s, Test Loss: 1.4534, Test Acc: 0.4700, Inference Time/Batch: 0.0469s\n",
      "Epoch 4: Train Loss: 1.3746, Train Acc: 0.4976, Train Time/Batch: 0.0547s, Test Loss: 1.3515, Test Acc: 0.5110, Inference Time/Batch: 0.0472s\n",
      "Epoch 5: Train Loss: 1.2775, Train Acc: 0.5373, Train Time/Batch: 0.0547s, Test Loss: 1.2673, Test Acc: 0.5380, Inference Time/Batch: 0.0467s\n",
      "Epoch 6: Train Loss: 1.2016, Train Acc: 0.5648, Train Time/Batch: 0.0553s, Test Loss: 1.2282, Test Acc: 0.5542, Inference Time/Batch: 0.0464s\n",
      "Epoch 7: Train Loss: 1.1350, Train Acc: 0.5897, Train Time/Batch: 0.0546s, Test Loss: 1.2031, Test Acc: 0.5654, Inference Time/Batch: 0.0464s\n",
      "Epoch 8: Train Loss: 1.0786, Train Acc: 0.6134, Train Time/Batch: 0.0551s, Test Loss: 1.1835, Test Acc: 0.5783, Inference Time/Batch: 0.0463s\n",
      "Epoch 9: Train Loss: 1.0322, Train Acc: 0.6304, Train Time/Batch: 0.0547s, Test Loss: 1.1433, Test Acc: 0.5929, Inference Time/Batch: 0.0461s\n",
      "Epoch 10: Train Loss: 0.9871, Train Acc: 0.6472, Train Time/Batch: 0.0550s, Test Loss: 1.1133, Test Acc: 0.6041, Inference Time/Batch: 0.0463s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_performer_phi_plus_noROPE.pth\n",
      "\n",
      "Summary for vision_performer_phi_plus_noROPE:\n",
      "Final Test Accuracy: 0.6041\n",
      "Accuracy Variance: 0.0059\n",
      "Average Training Time per Batch: 0.0549s\n",
      "Average Inference Time per Batch: 0.0465s\n"
     ]
    }
   ],
   "source": [
    "# Performer with Phi+ No ROPE\n",
    "model_performer_phi_plus = VisionTransformerPerformer(use_phi_plus_plus=False).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plus.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plus, \"vision_performer_phi_plus_noROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7aac5ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_performer_phi_plus_plus_noROPE...\n",
      "Epoch 1: Train Loss: 1.8609, Train Acc: 0.3095, Train Time/Batch: 0.0617s, Test Loss: 1.6650, Test Acc: 0.3903, Inference Time/Batch: 0.0613s\n",
      "Epoch 2: Train Loss: 1.5708, Train Acc: 0.4209, Train Time/Batch: 0.0616s, Test Loss: 1.4843, Test Acc: 0.4582, Inference Time/Batch: 0.0594s\n",
      "Epoch 3: Train Loss: 1.4266, Train Acc: 0.4767, Train Time/Batch: 0.0617s, Test Loss: 1.4016, Test Acc: 0.4954, Inference Time/Batch: 0.0592s\n",
      "Epoch 4: Train Loss: 1.3236, Train Acc: 0.5147, Train Time/Batch: 0.0617s, Test Loss: 1.3252, Test Acc: 0.5211, Inference Time/Batch: 0.0596s\n",
      "Epoch 5: Train Loss: 1.2353, Train Acc: 0.5520, Train Time/Batch: 0.0618s, Test Loss: 1.2858, Test Acc: 0.5300, Inference Time/Batch: 0.0591s\n",
      "Epoch 6: Train Loss: 1.1573, Train Acc: 0.5806, Train Time/Batch: 0.0619s, Test Loss: 1.2651, Test Acc: 0.5474, Inference Time/Batch: 0.0595s\n",
      "Epoch 7: Train Loss: 1.0896, Train Acc: 0.6072, Train Time/Batch: 0.0619s, Test Loss: 1.1879, Test Acc: 0.5765, Inference Time/Batch: 0.0589s\n",
      "Epoch 8: Train Loss: 1.0319, Train Acc: 0.6313, Train Time/Batch: 0.0615s, Test Loss: 1.2041, Test Acc: 0.5696, Inference Time/Batch: 0.0591s\n",
      "Epoch 9: Train Loss: 0.9770, Train Acc: 0.6501, Train Time/Batch: 0.0614s, Test Loss: 1.1585, Test Acc: 0.5870, Inference Time/Batch: 0.0579s\n",
      "Epoch 10: Train Loss: 0.9232, Train Acc: 0.6679, Train Time/Batch: 0.0616s, Test Loss: 1.1688, Test Acc: 0.5850, Inference Time/Batch: 0.0586s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_performer_phi_plus_plus_noROPE.pth\n",
      "\n",
      "Summary for vision_performer_phi_plus_plus_noROPE:\n",
      "Final Test Accuracy: 0.5850\n",
      "Accuracy Variance: 0.0036\n",
      "Average Training Time per Batch: 0.0617s\n",
      "Average Inference Time per Batch: 0.0593s\n"
     ]
    }
   ],
   "source": [
    "# Performer with Phi++ No ROPE\n",
    "model_performer_phi_plus_plus = VisionTransformerPerformer(use_phi_plus_plus=True).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plus_plus.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plus_plus, \"vision_performer_phi_plus_plus_noROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "84094f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_performer_phi_plus_axial_nontrain_ROPE...\n",
      "Epoch 1: Train Loss: 1.9692, Train Acc: 0.2680, Train Time/Batch: 0.0941s, Test Loss: 1.7996, Test Acc: 0.3319, Inference Time/Batch: 0.0907s\n",
      "Epoch 2: Train Loss: 1.6552, Train Acc: 0.3861, Train Time/Batch: 0.0941s, Test Loss: 1.5581, Test Acc: 0.4248, Inference Time/Batch: 0.0908s\n",
      "Epoch 3: Train Loss: 1.5211, Train Acc: 0.4386, Train Time/Batch: 0.0940s, Test Loss: 1.5272, Test Acc: 0.4433, Inference Time/Batch: 0.0900s\n",
      "Epoch 4: Train Loss: 1.4366, Train Acc: 0.4754, Train Time/Batch: 0.0947s, Test Loss: 1.4260, Test Acc: 0.4808, Inference Time/Batch: 0.0903s\n",
      "Epoch 5: Train Loss: 1.3644, Train Acc: 0.5043, Train Time/Batch: 0.0941s, Test Loss: 1.3772, Test Acc: 0.4979, Inference Time/Batch: 0.0905s\n",
      "Epoch 6: Train Loss: 1.3156, Train Acc: 0.5221, Train Time/Batch: 0.0939s, Test Loss: 1.3227, Test Acc: 0.5224, Inference Time/Batch: 0.0927s\n",
      "Epoch 7: Train Loss: 1.2664, Train Acc: 0.5419, Train Time/Batch: 0.0934s, Test Loss: 1.2862, Test Acc: 0.5382, Inference Time/Batch: 0.0895s\n",
      "Epoch 8: Train Loss: 1.2235, Train Acc: 0.5601, Train Time/Batch: 0.0935s, Test Loss: 1.2616, Test Acc: 0.5451, Inference Time/Batch: 0.0895s\n",
      "Epoch 9: Train Loss: 1.1935, Train Acc: 0.5709, Train Time/Batch: 0.0933s, Test Loss: 1.2393, Test Acc: 0.5551, Inference Time/Batch: 0.0890s\n",
      "Epoch 10: Train Loss: 1.1561, Train Acc: 0.5852, Train Time/Batch: 0.0940s, Test Loss: 1.2019, Test Acc: 0.5682, Inference Time/Batch: 0.0901s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_performer_phi_plus_axial_nontrain_ROPE.pth\n",
      "\n",
      "Summary for vision_performer_phi_plus_axial_nontrain_ROPE:\n",
      "Final Test Accuracy: 0.5682\n",
      "Accuracy Variance: 0.0048\n",
      "Average Training Time per Batch: 0.0939s\n",
      "Average Inference Time per Batch: 0.0903s\n"
     ]
    }
   ],
   "source": [
    "# Axial RoPE（Non-trainable theata） + Performer Phi+\n",
    "model_performer_phi_plus_axial_rope_nontrain = VisionTransformerPerformer(axialRoPE=True, trainable_theta=False).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plus_axial_rope_nontrain.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plus_axial_rope_nontrain, \"vision_performer_phi_plus_axial_nontrain_ROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ec64ef1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_performer_phi_plus_axial_trained_ROPE...\n",
      "Epoch 1: Train Loss: 1.9689, Train Acc: 0.2659, Train Time/Batch: 0.1167s, Test Loss: 1.8169, Test Acc: 0.3226, Inference Time/Batch: 0.0909s\n",
      "Epoch 2: Train Loss: 1.6856, Train Acc: 0.3758, Train Time/Batch: 0.1187s, Test Loss: 1.5975, Test Acc: 0.4163, Inference Time/Batch: 0.0979s\n",
      "Epoch 3: Train Loss: 1.5419, Train Acc: 0.4292, Train Time/Batch: 0.1189s, Test Loss: 1.5330, Test Acc: 0.4381, Inference Time/Batch: 0.0925s\n",
      "Epoch 4: Train Loss: 1.4518, Train Acc: 0.4621, Train Time/Batch: 0.1140s, Test Loss: 1.4539, Test Acc: 0.4686, Inference Time/Batch: 0.0888s\n",
      "Epoch 5: Train Loss: 1.3868, Train Acc: 0.4908, Train Time/Batch: 0.1169s, Test Loss: 1.3940, Test Acc: 0.4891, Inference Time/Batch: 0.0955s\n",
      "Epoch 6: Train Loss: 1.3258, Train Acc: 0.5160, Train Time/Batch: 0.1154s, Test Loss: 1.3289, Test Acc: 0.5194, Inference Time/Batch: 0.0918s\n",
      "Epoch 7: Train Loss: 1.2802, Train Acc: 0.5344, Train Time/Batch: 0.1154s, Test Loss: 1.3000, Test Acc: 0.5328, Inference Time/Batch: 0.0917s\n",
      "Epoch 8: Train Loss: 1.2354, Train Acc: 0.5521, Train Time/Batch: 0.1182s, Test Loss: 1.2680, Test Acc: 0.5424, Inference Time/Batch: 0.0908s\n",
      "Epoch 9: Train Loss: 1.2000, Train Acc: 0.5664, Train Time/Batch: 0.1154s, Test Loss: 1.2575, Test Acc: 0.5509, Inference Time/Batch: 0.0939s\n",
      "Epoch 10: Train Loss: 1.1683, Train Acc: 0.5794, Train Time/Batch: 0.1155s, Test Loss: 1.2124, Test Acc: 0.5709, Inference Time/Batch: 0.0927s\n",
      "Model saved at: ./saved_models_CIFAR10\\vision_performer_phi_plus_axial_trained_ROPE.pth\n",
      "\n",
      "Summary for vision_performer_phi_plus_axial_trained_ROPE:\n",
      "Final Test Accuracy: 0.5709\n",
      "Accuracy Variance: 0.0052\n",
      "Average Training Time per Batch: 0.1165s\n",
      "Average Inference Time per Batch: 0.0927s\n"
     ]
    }
   ],
   "source": [
    "# Axial RoPE（Trainable theata） + Performer Phi+\n",
    "model_performer_phi_plus_axial_rope_train = VisionTransformerPerformer(axialRoPE=True, trainable_theta=True ).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plus_axial_rope_train.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plus_axial_rope_train, \"vision_performer_phi_plus_axial_trained_ROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e9bc1b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating vision_performer_phi_plusplus_axial_nontrain_ROPE...\n",
      "Input shape: torch.Size([64, 65, 64])\n",
      "Q, K, V shapes after split: torch.Size([64, 65, 8, 8]), torch.Size([64, 65, 8, 8]), torch.Size([64, 65, 8, 8])\n",
      "Reshaped Q, K, V: torch.Size([512, 65, 8]), torch.Size([512, 65, 8]), torch.Size([512, 65, 8])\n",
      "Q_prime, K_prime after phi: torch.Size([512, 65, 16]), torch.Size([512, 65, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512, 65, 16]' is invalid for input of size 1064960",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[241], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_performer_phi_plusplus_axial_rope_nontrain \u001b[38;5;241m=\u001b[39m VisionTransformerPerformer(axialRoPE\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trainable_theta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_phi_plus_plus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_performer_phi_plusplus_axial_rope_nontrain\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mevaluate_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_performer_phi_plusplus_axial_rope_nontrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvision_performer_phi_plusplus_axial_nontrain_ROPE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 72\u001b[0m, in \u001b[0;36mevaluate_framework\u001b[1;34m(model, model_name, train_loader, test_loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     69\u001b[0m train_times, inference_times \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 72\u001b[0m     train_loss, train_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     test_loss, test_acc, avg_inference_time \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion)\n\u001b[0;32m     75\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m, in \u001b[0;36mtrain_with_speed\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     50\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 52\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, y)\n\u001b[0;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[240], line 200\u001b[0m, in \u001b[0;36mVisionTransformerPerformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    197\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperformer_layers:\n\u001b[1;32m--> 200\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(x[:, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[240], line 249\u001b[0m, in \u001b[0;36mTransformerBlockPerformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03mForward pass of the transformer block using Performer attention.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m- Tensor: Output tensor of the same shape as input after passing through the transformer block.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Apply self-attention and add the result to the input (residual connection)\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsa_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Apply the feed-forward network and add the result to the previous output (residual connection)\u001b[39;00m\n\u001b[0;32m    252\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm(x))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[240], line 91\u001b[0m, in \u001b[0;36mPerformerSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m         k_prime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxial_rope(k_prime, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth)  \u001b[38;5;66;03m# Applying trainable RoPE to keys\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         q_prime \u001b[38;5;241m=\u001b[39m \u001b[43mapply_axial_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_head_dim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Applying non-trainable RoPE\u001b[39;00m\n\u001b[0;32m     92\u001b[0m         k_prime \u001b[38;5;241m=\u001b[39m apply_axial_rope(k_prime, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth, actual_head_dim)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ_prime, K_prime after RoPE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_prime\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_prime\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mapply_axial_rope\u001b[1;34m(embeddings, height, width, dim, trainable, theta_h, theta_w)\u001b[0m\n\u001b[0;32m     43\u001b[0m rotated_embeddings[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m x1_h_rotated \u001b[38;5;241m+\u001b[39m x1_w_rotated\n\u001b[0;32m     44\u001b[0m rotated_embeddings[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m x2_h_rotated \u001b[38;5;241m+\u001b[39m x2_w_rotated\n\u001b[1;32m---> 46\u001b[0m rotated_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mrotated_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rotated_embeddings\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[512, 65, 16]' is invalid for input of size 1064960"
     ]
    }
   ],
   "source": [
    "# Axial RoPE（Non-trainable theata） + Performer Phi++\n",
    "model_performer_phi_plusplus_axial_rope_nontrain = VisionTransformerPerformer(axialRoPE=True, trainable_theta=False, use_phi_plus_plus=True).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plusplus_axial_rope_nontrain.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plusplus_axial_rope_nontrain, \"vision_performer_phi_plusplus_axial_nontrain_ROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bf74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial RoPE（Trainable theata） + Performer Phi++\n",
    "model_performer_phi_plusplus_axial_rope_train = VisionTransformerPerformer(axialRoPE=True, trainable_theta=True,use_phi_plus_plus=True).to(device)\n",
    "optimizer = optim.Adam(model_performer_phi_plusplus_axial_rope_train.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_performer_phi_plusplus_axial_rope_train, \"vision_performer_phi_plusplus_axial_trained_ROPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9179e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU with Gaussian matrix, no RoPE\n",
    "model_ReLU_Gaussian = VisionTransformerPerformer(use_gaussian_performer=True).to(device)\n",
    "optimizer = optim.Adam(model_ReLU_Gaussian.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_ReLU_Gaussian, \"model_ReLU_Gaussian_no_RoPE\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdb6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial RoPE（Non-trainable theata）+ ReLU with Gaussian matrix\n",
    "ReLU_Gaussian_axialrope_nontraintheta = VisionTransformerPerformer(use_gaussian_performer=True,axialRoPE=True, trainable_theta=False).to(device)\n",
    "optimizer = optim.Adam(ReLU_Gaussian_axialrope_nontraintheta.parameters(), lr=3e-4)\n",
    "evaluate_framework(ReLU_Gaussian_axialrope_nontraintheta, \"model_ReLU_Gaussian_axialRoPE_nontrain_theata\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a339d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial RoPE（trainable theata）+ ReLU with Gaussian matrix\n",
    "model_ReLU_Gaussian_axialrope_nontraintheta = VisionTransformerPerformer(use_gaussian_performer=True,axialRoPE=True, trainable_theta=True).to(device)\n",
    "optimizer = optim.Adam(model_ReLU_Gaussian_axialrope_nontraintheta.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_ReLU_Gaussian_axialrope_nontraintheta, \"model_ReLU_Gaussian_axialRoPE_trainable_theta\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU with Learnable G matrix, no RoPE\n",
    "model_ReLU_LearnableG = VisionTransformerPerformer(use_gaussian_performer=True, learnable_G=True).to(device)\n",
    "optimizer = optim.Adam(model_ReLU_LearnableG.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_ReLU_LearnableG, \"model_ReLU_Learnable_G_no_RoPE\", train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fcb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial RoPE（Non-trainable theata）+ ReLU with Learnable G\n",
    "ReLU_LearnableG_axialrope_nontraintheta = VisionTransformerPerformer(use_gaussian_performer=True,axialRoPE=True, trainable_theta=False, learnable_G=True).to(device)\n",
    "optimizer = optim.Adam(ReLU_LearnableG_axialrope_nontraintheta.parameters(), lr=3e-4)\n",
    "evaluate_framework(ReLU_LearnableG_axialrope_nontraintheta, \"model_LearnableG_axialRoPE_nontrain_theata\", train_loader, test_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial RoPE（trainable theata）+ ReLU with  with Learnable G\n",
    "model_ReLU_LearnableG_axialrope_traintheta = VisionTransformerPerformer(use_gaussian_performer=True,axialRoPE=True, trainable_theta=True, learnable_G=True).to(device)\n",
    "optimizer = optim.Adam(model_ReLU_LearnableG_axialrope_traintheta.parameters(), lr=3e-4)\n",
    "evaluate_framework(model_ReLU_LearnableG_axialrope_traintheta, \"model_ReLU_LearnableG_axialRoPE_trainable_theta\", train_loader, test_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77570d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
